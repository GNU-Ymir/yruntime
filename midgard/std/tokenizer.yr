mod std::tokenizer;

import core::object;
import core::array;
import core::typeinfo;
import core::exception;

import std::collection::map;
import std::collection::vec;
import std::io;

/**
 * A tokenizer is an enhanced string splitter, that splits strings using tokens instead of just chars
 * Tokenizer are really usefull for grammar visitor, and can be associated with Lexers pretty easily
 * @example: 
 * ============
 * // using a tokenizer, tokens can be multiple letter long, and there can be collision between tokens
 * // For example, the token '=' and '=>' won't be a problem for the tokenizer
 * let dmut tzer = Tokenizer::new (tokens-> ["(", ")", "=>", ",", " ", "="]);
 * let str = "(x, y) => x = y";
 * let lst = tzer.tokenize (str);
 * 
 * // the tokenized str, contains the token '=>' and '=', that are correctly rendering in the splitted array 
 * assert (lst == ["(", "x", ",", " ", "y", ")", " ", "=>", " ", "x", " ", "=", " ", "y"]); 
 * ============
 */
pub class @final Tokenizer {

    prv let dmut _heads = HashMap!(c32, &internal::Node)::new ();

    /**
     * Create a new tokenizer, with a set of tokens
     * @params: 
     *   - tokens: the list of token that will split the string
     * @example: 
     * ============
     * let dmut tzer = Tokenizer::new (tokens-> ["(", ")", "=>", ":", "<", ">", ",", " "]);
     * let str = "(x, y) => x > y";
     * let lst = tzer.tokenize (str);
     * assert (lst == ["(", "x", ",", " ", "y", ")", " ", "=>", " ", "x", " ", ">", " ", "y"]); 
     * ============
     */
    pub self (tokens: [[c32]] = []) {
        for i in tokens {
            self:.insert (i);
        }
    }

    /**
     * Insert a new token in the tokenizer
     * @params: 
     *    - token: the token to insert
     * @example: 
     * ================
     * let dmut tzer = Tokenizer::new ();
     * tzer:.insert ("+");
     * tzer:.insert ("+=");
     * tzer:.insert (" ");
     * let lst = tzer.tokenize ("x += y");
     * assert (lst == ["x", " ", "+=", " ", "y"]);
     * ================
     */
    pub def insert (mut self, token : [c32]) {
        if (token.len != 0u64) {
            {
                let fnd = (self._heads [token[0]])?;
                match fnd {
                    Ok (x:_) => {
                        (alias self._heads) [token[0]] = x.insert (token[1u64..$]);                    
                    }
                    _ => {
                        (alias self._heads) [token[0]] = internal::Node::new (token[0]).insert (token [1u64..$]);
                    }
                }
            } catch {
                _ : &OutOfArray => { } // impossible
            }
        }
    }

    /**
     * @returns: the length of the next token inside the str
     * @example: 
     * ============
     * let dmut tzer = Tokenizer::new (["+", " "]);
     * let mut str = "fst + scd";
     * let mut len = tzer.next (str);
     * assert (len == 3u64); // "fst"
     * 
     * str = str [len .. $];     
     * len = tzer.next (str);
     * assert (len == 1u64); // " "
     *
     * str = str [len .. $];     
     * len = tzer.next (str);
     * assert (len == 1u64); // "+"
     *
     * str = str [len .. $];     
     * len = tzer.next (str);
     * assert (len == 1u64); // " "
     *
     * str = str [len .. $];     
     * len = tzer.next (str);
     * assert (len == 3u64); // "scd" 
     *
     * str = str [len .. $];     
     * len = tzer.next (str);
     * assert (len == 0u64); 
     * ============
     */
    pub def next (self, str : [c32])-> u64 {
        for i in 0u64 .. str.len {
            let fnd = (self._heads [str [i]])?;
            {
                match fnd { 
                    Ok (x:_) => { // a possible token at index == i
                        let len = x.len (str [1u64 .. $]); // get the length of the token
                        // if the len is 0, then it is not really a token, it just start like one
                        if (len != 0u64) {
                            if (i == 0u64) { 
                                return len; // it is totally a token, we return its length
                            } else {
                                // it is a token, but there is something before it, so we return the len of the thing before it
                                return i; 
                            }
                        }
                        // it was not a token, just started like one, we continue
                    }
                }
            } catch {
                _ : &OutOfArray=>  {} // impossible
            }
        }
        
        // No token in the str, return the len of the str
        return str.len;
    }

    /**
     * Split the string in a list of token, according to the token registered in the tokenizer
     * @example: 
     * =============== 
     * let dmut tzer = Tokenizer::new (tokens-> ["+=", "+", " "]);     * 
     * 
     * let lst = tzer.tokenize ("x += y");
     * assert (lst == ["x", " ", "+=", " ", "y"]);
     * ===============
     */
    pub def tokenize (self, str : [c32])-> [[c32]] {
        let dmut res = Vec!([c32])::new ();
        let mut aux = str;
        { 
            while aux.len > 0u64 {
                let len = self.next (aux);
                res:.push (aux [0u64..len]);
                aux = aux [len .. $];
            }
        } catch {
            _: &OutOfArray=> {}
        }
        
        res:.fit ();
        return res [];
    }

    impl std::io::Printable;
    
}

mod internal {

    /**
     * A node of a tokenizer, that stores information about tokens
     */
    pub class @final Node {

        // The current value of the node
        let _key : c32; 

        // Can terminate a token? or is part of bigger tokens
        let _isToken : bool = false;

        // The list of possible continuation of the token
        let _heads : &HashMap!(c32, &Node);

        /**
         * Construct a new Token node
         * @params: 
         *   - key: the value of the node
         *   - isToken: can terminate a token 
         *   - heads: the list of possible continuation
         */
        pub self (key : c32, isToken : bool = false, heads : &HashMap!(c32, &Node) = {HashMap!(c32, &Node)::new ()}) with _key = key, _isToken = isToken, _heads = heads
        {}
        
        /**
         * Insert sub tokens accepted tokens
         * @params: 
         *     - str: the rest to read to create a valid token
         * @example: 
         * ==============
         * // let say that "[+]" is a token, but "[" is not, nor is "[+"
         * let mut node = Node::new ('['); 
         * node = node.insert ("+]"); 
         * println (node); // [:false, +:false, ]:true 
         * // In that configuration the only token that will be accepted is "[+]"
         * assert (node.len ("[+]") == 3); // accepted
         * assert (node.len ("[") == 0u64); // not accepted
         * assert (node.len ("[+") == 0u64); // not accepted
         * 
         * // Now we want to accept "[-]"
         * node = node.insert ("-]");
         * // and simply "["
         * node = node.insert (""); 
         * println (node); // [:true, 
         *                 //     +:false, ]:true 
         *                 //     -: false, ]:true
         * 
         * assert (node.len ("[+]") == 3); // still accepted
         * assert (node.len ("[") == 1u64); // accepted this time
         * assert (node.len ("[+") == 1u64); // accept only the '['
         * assert (node.len ("[-]") == 3); // accepted
         * assert (node.len ("[-") == 1u64); // accept only the '['
         * ==============
         */
        pub def insert (self, str : [c32]) -> &Node {
            if (str.len == 0u64) {
                return Node::new (self._key, isToken-> true, heads-> self._heads)
            }

            let dmut retDict = HashMap!(c32, &Node)::new ();
            for i, j in self._heads {
                retDict:.insert (i, j);                    
            }

            let fnd = retDict[str [0]]?
            {
                match fnd {
                    Ok (x:_) => {
                        retDict:.insert (str [0], x.insert (str [1u64..$]));
                    }
                    _ => {                    
                        retDict:.insert (str [0], Node::new (str [0]).insert (str [1u64..$]));
                    }
                }
            } catch {
                _: &OutOfArray => {}
            }
            
            return Node::new (self._key, isToken-> self._isToken, heads-> retDict);
        }

            

        /**
         * @returns: the length of the token at the beginning of the string content
         * @example: 
         * =================
         * let mut node = Node::new ('+', isToken-> true);
         * node = node.insert ("=");
         * // Our grammar accept the tokens, "+" and "+="
         * assert (node.len ("+") == 1u64); // "+" are accepted
         * assert (node.len ("+=") == 2u64); // "+=" are accepted
         * assert (node.len (" +=") == 0u64); // " +=" are not accepted
         * assert (node.len ("+ and some rest") == 1u64); // " +" are accepted
         * =================
         */
        pub def len (self, content : [c32])-> u64 {
            if (content.len == 0u64) {
                if (self._isToken) return 1u64;
                else return 0u64;
            }

            let fnd = (self._heads [content [0]])?
            match fnd {
                Ok (x:_) => {
                    let sub_len = x.len (content [1u64..$]);
                    if (sub_len != 0u64 || self._isToken) return 1u64 + sub_len;
                    else return 0u64;                
                } catch {
                    _: &OutOfArray => {} // impossible
                }
            }

            if (self._isToken) return 1u64;
            return 0u64;
        }

        /**
         * @returns: the key of the node
         */
        def key (self) -> c32 {
            self._key
        }

        impl std::io::Printable {
            /**
             * For debug purposes, tokenizer Nodes are printable
             */
            pub over print (self) {
                self.innerPrint (0);
            }
        }

        /**
         * Debug print of the node on stdout
         */
        prv def innerPrint (self, i : i32) {
            for _ in 0 .. i print ("\t");            
            println ('[', self._key, ']', self._isToken);
            for _, j in self._heads {                
                j.innerPrint (i + 1);                
            }            
        }
        
    }

    
}



