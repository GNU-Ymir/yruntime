/**
 * This module implements the Tokenizer class that splits a string into a list of token.
 * The Tokenizer always splits according to the longest tokens, for example if the tokens `"+=>"`, `"+="`, `"+"` and `" "` are defined, the string `"+=> x"` will be splitted in 3 tokens `"+=>"`, `" "` and `"x"`.
 * This implementation uses an index tree to store the tokens, where each node is a single char. Tokenizer should be used for simple grammar, for complex branching decision a Lexer should be preferred - in practice the lexer uses a Tokenizer, but with more abstractions, facilitating the work and making sure no tokenization are made when not needed.
 * @Authors: Emile Cadorel
 * @License: GPLv3
 * <hr>
 * @example
 * ===
 * import std::syntax::tokenizer;
 * 
 * // using a tokenizer, tokens can be multiple letter long, and there can be collisions between tokens
 * // For example, the token '=' and '=>' won't be a problem for the tokenizer
 * let dmut tzer = Tokenizer::new (tokens-> ["(", ")", "=>", ",", " ", "="]);
 * let str = "(x, y) => x = y";
 * 
 * // Transform the string into a list of tokens
 * let lst = tzer.tokenize (str);
 * 
 * // the tokenized str, contains the token '=>' and '=', that are correctly rendered in the splitted array 
 * assert (lst == ["(", "x", ",", " ", "y", ")", " ", "=>", " ", "x", " ", "=", " ", "y"]); 
 * ===
 */

mod std::syntax::tokenizer;

import core::object;
import core::array;
import core::typeinfo;
import core::exception;
import core::dispose;
import core::duplication;

import std::collection::map;
import std::collection::vec;
import std::io, std::stream;

/**
 * Implementation of a string Tokenizer, that can split a utf8 or utf32 string.
 */
pub class if (is!T {U of c8} || is!T {U of c32}) @final Tokenizer {T} {

    prv let dmut _heads = HashMap!{T, &internal::Node!{T}}::new ();

    /**
     * Create a new tokenizer, with a set of tokens
     * @params: 
     *   - tokens: the list of token that will split the string
     * @example: 
     * ============
     * import std::syntax::tokenizer;
     * 
     * let dmut tzer = Tokenizer::new (tokens-> ["(", ")", "=>", ":", "<", ">", ",", " "]);
     * 
     * let str = "(x, y) => x > y";
     * 
     * // Transform the string into a list of tokens
     * let lst = tzer.tokenize (str);
     * 
     * assert (lst == ["(", "x", ",", " ", "y", ")", " ", "=>", " ", "x", " ", ">", " ", "y"]); 
     * ============
     */
    pub self (tokens: [[T]] = []) {
        for i in tokens {
            self:.insert (i);
        }
    }

    /**
     * Insert a new token in the tokenizer
     * @params: 
     *    - token: the token to insert
     *    - isSkip: flag the token as a skip token
     *    - isComment: flag the token as a comment token, where isComment is the token that ends the comment
     * @example: 
     * ================
     * import std::syntax::tokenizer;
     * 
     * // cannot use the 'tokens' parameter of ctor to guess the type of the tokenizer
     * //  so we have to put the template parameter
     * let dmut tzer = Tokenizer!{c32}::new ();
     * 
     * tzer:.insert ("+");
     * tzer:.insert ("+=");
     * tzer:.insert (" ");
     * 
     * let lst = tzer.tokenize ("x += y");
     * assert (lst == ["x", " ", "+=", " ", "y"]);
     * ================
     */
    pub fn insert (mut self, token : [T], isSkip : bool = false, isComment : [T] = []) {
        if (token.len != 0us) {
            {
                let fnd = (self._heads [token[0]])?;
                match fnd {
                    Ok (x:_) => {
                        (alias self._heads) [token[0]] = x.insert (token[1us..$], isSkip-> isSkip, isComment-> isComment);                    
                    }
                    _ => {
                        (alias self._heads) [token[0]] = internal::Node!{T}::new (token[0]).insert (token [1us..$], isSkip-> isSkip, isComment-> isComment);
                    }
                }
            }
        }
    }

    /**
     * @returns: the length of the next token inside the str
     * @example: 
     * ============
     * import std::syntax::tokenizer;
     * 
     * let dmut tzer = Tokenizer::new (tokens-> ["+", " "]);
     * let mut str = "fst + scd";
     * let mut len = tzer.next (str);
     * assert (len == 3us); // "fst"
     * 
     * // change the slice to move the cursor     
     * str = str [len .. $]; 
     * len = tzer.next (str);
     * assert (len == 1us); // " "
     *
     * str = str [len .. $];     
     * len = tzer.next (str);
     * assert (len == 1us); // "+"
     *
     * str = str [len .. $];     
     * len = tzer.next (str);
     * assert (len == 1us); // " "
     *
     * str = str [len .. $];     
     * len = tzer.next (str);
     * assert (len == 3us); // "scd" 
     *
     * str = str [len .. $];     
     * len = tzer.next (str);
     * assert (len == 0us); 
     * ============
     */
    pub fn next (self, str : [T])-> usize {
        for i in 0us .. str.len {
            let fnd = (self._heads [str [i]])?;
            {
                match fnd { 
                    Ok (x:_) => { // a possible token at index == i
                        let len = x.len (str [i + 1us .. $]); // get the length of the token
                        // if the len is 0, then it is not really a token, it just start like one
                        if (len != 0us) {
                            if (i == 0us) { 
                                return len; // it is totally a token, we return its length
                            } else {
                                // it is a token, but there is something before it, so we return the len of the thing before it
                                return i; 
                            }
                        }
                        // it was not a token, just started like one, we continue
                    }
                }
            }
        }
        
        // No token in the str, return the len of the str
        return str.len;
    }

    /**
     * Perform the same treatment as self.next, but also returns the flag of the token that was read.
     * @returns: 
     *   - .0: the length of the next token inside the str
     *   - .1: true if the token is a skip token
     *   - .2: the end token if the returned token is a comment token
     */
    pub fn nextWithFlags (self, str : [T])-> (usize, bool, [T]) {
        for i in 0us .. str.len {
            let fnd = (self._heads [str [i]])?;
            {
                match fnd { 
                    Ok (x:_) => { // a possible token at index == i
                        let len = x.completeLen (str [i + 1us .. $]); // get the length of the token
                        // if the len is 0, then it is not really a token, it just start like one
                        if (len._0 != 0us) {
                            if (i == 0us) { 
                                return len; // it is totally a token, we return its length
                            } else {
                                // it is a token, but there is something before it, so we return the len of the thing before it
                                return (i, false, []); 
                            }
                        }
                        // it was not a token, just started like one, we continue
                    }
                }
            }
        }
        
        // No token in the str, return the len of the str
        return (str.len, false, []);
    }

    /**
     * Split the string using the list of token registered in the tokenizer.
     * @example: 
     * =============== 
     * let dmut tzer = Tokenizer::new (tokens-> ["+=", "+", " "]);
     * 
     * let lst = tzer.tokenize ("x += y");
     * assert (lst == ["x", " ", "+=", " ", "y"]);
     * ===============
     */
    pub fn tokenize (self, str : [T])-> [[T]] {
        let dmut res = Vec!{[T]}::new ();
        let mut aux = str;
        { 
            while aux.len > 0us {
                let len = self.next (aux);
                res:.push (aux [0us..len]);
                aux = aux [len .. $];
            }
        }
        
        res:.fit ();
        return res [];
    }

    /**
     * Split the string using the list of token registered in the tokenizer, and add the flags of the tokens.
     * @returns: An array of elements being : 
     *    - .0: the token
     *    - .1: true iif the token is flagged as a skip token
     *    - .2: iif the token is a comment start, the token ending the comment, "" otherwise
     * @example: 
     * =============== 
     * let dmut tzer = Tokenizer::new (tokens-> ["+=", "+"]);
     * tzer:.insert (" ", isSkip-> true);
     * tzer:.insert ("#", isComment-> "\n");
     * 
     * let lst = tzer.tokenizeWithFlags ("x += #");
     *
     * // Second token is a space token, and it was marked as skippable
     * assert (lst[1]._0 == " " && lst[1]._1 == true);    
     * 
     * // Last token is '#' token, and it was flagged as being a comment, ending with '\n'
     * assert (lst[$ - 1us]._0 == "#" && lst[$ - 1us]._2 == "\n");
     * ===============
     */
    pub fn tokenizeWithFlags (self, str : [T])-> [([T], bool, [T])] {
        let dmut res = Vec!{([T], bool, [T])}::new ();
        let mut aux = str;
        { 
            while aux.len > 0us {
                let (len, isSkip, isComment) = self.nextWithFlags (aux);
                res:.push ((aux [0us..len], isSkip, isComment));
                aux = aux [len .. $];
            }
        }
        
        res:.fit ();
        return res [];        
    }
    
    impl std::stream::Streamable, core::duplication::Copiable;    
    
}

mod internal {

    /**
     * A node of a tokenizer, that stores information about tokens
     */
    pub class @final Node {T} {

        // The current value of the node
        let _key : T; 

        // Can terminate a token? or is part of bigger tokens
        let _isToken : bool = false;

        // The end of the comment if this token is the start of a comment
        let _isComment : [T]; 

        // True if the token is a token to skip
        let _isSkip : bool; 
        
        // The list of possible continuation of the token
        let _heads : &HashMap!{T, &Node!{T}};

        /**
         * Construct a new Token node
         * @params: 
         *   - key: the value of the node
         *   - isToken: can terminate a token 
         *   - heads: the list of possible continuation
         */
        pub self (key : T, isToken : bool = false, isSkip : bool = false, isComment : [T] = [], heads : &HashMap!{T, &Node!{T}} = {HashMap!{T, &Node!{T}}::new ()}) with _key = key, _isToken = isToken, _heads = heads, _isSkip = isSkip, _isComment = isComment
        {}
        
        /**
         * Insert sub tokens accepted tokens
         * @params: 
         *     - str: the rest to read to create a valid token
         * @example: 
         * ==============
         * // let say that "[+]" is a token, but "[" is not, nor is "[+"
         * let mut node = Node::new ('['); 
         * node = node.insert ("+]"); 
         * println (node); // [:false, +:false, ]:true 
         * // In that configuration the only token that will be accepted is "[+]"
         * assert (node.len ("[+]") == 3); // accepted
         * assert (node.len ("[") == 0us); // not accepted
         * assert (node.len ("[+") == 0us); // not accepted
         * 
         * // Now we want to accept "[-]"
         * node = node.insert ("-]");
         * // and simply "["
         * node = node.insert (""); 
         * println (node); // [:true, 
         *                 //     +:false, ]:true 
         *                 //     -: false, ]:true
         * 
         * assert (node.len ("[+]") == 3); // still accepted
         * assert (node.len ("[") == 1us); // accepted this time
         * assert (node.len ("[+") == 1us); // accept only the '['
         * assert (node.len ("[-]") == 3); // accepted
         * assert (node.len ("[-") == 1us); // accept only the '['
         * ==============
         */
        pub fn insert (self, str : [T], isSkip : bool = false, isComment : [T] = []) -> &Node!{T} {
            if (str.len == 0us) {
                return Node::new (self._key, isToken-> true, heads-> self._heads, isSkip-> isSkip, isComment-> isComment)
            }

            let dmut retDict = HashMap!{T, &Node!{T}}::new ();
            for i, j in self._heads {
                retDict:.insert (i, j);                    
            }

            let fnd = retDict[str [0]]?;
            {
                match fnd {
                    Ok (x:_) => {
                        retDict:.insert (str [0], x.insert (str [1us..$], isSkip-> isSkip, isComment-> isComment));
                    }
                    _ => {                    
                        retDict:.insert (str [0], Node::new (str [0]).insert (str [1us..$], isSkip-> isSkip, isComment-> isComment));
                    }
                }
            }
            
            return Node::new (self._key, isToken-> self._isToken, heads-> retDict, isSkip-> isSkip, isComment-> isComment);
        }

            

        /**
         * @returns: the length of the token at the beginning of the string content
         * @example: 
         * =================
         * let mut node = Node::new ('+', isToken-> true);
         * node = node.insert ("=");
         * // Our grammar accept the tokens, "+" and "+="
         * assert (node.len ("+") == 1us); // "+" are accepted
         * assert (node.len ("+=") == 2us); // "+=" are accepted
         * assert (node.len (" +=") == 0us); // " +=" are not accepted
         * assert (node.len ("+ and some rest") == 1us); // " +" are accepted
         * =================
         */
        pub fn len (self, content : [T])-> usize {
            if (content.len == 0us) {
                if (self._isToken) return 1us;
                else return 0us;
            }

            let fnd = (self._heads [content [0]])?;
            match fnd {
                Ok (x:_) => {
                    let sub_len = x.len (content [1us..$]);
                    if (sub_len != 0us || self._isToken) return 1us + sub_len;
                    else return 0us;                
                }
            }

            if (self._isToken) return 1us;
            return 0us;
        }

        /**
         * @returns: 
         *    - .0: the length of the token at the beginning of the string content
         *    - .1: true if this is a skip token
         *    - .2: the end of the comment if this is a skip token
         */
        pub fn completeLen (self, content : [T])-> (usize, bool, [T]) {
            if (content.len == 0us) {
                if (self._isToken) return (1us, self._isSkip, self._isComment);
                else return (0us, false, []);
            }

            let fnd = (self._heads [content [0]])?;
            match fnd {
                Ok (x:_) => {
                    let sub_len = x.completeLen (content [1us..$]);
                    if (sub_len._0 != 0us || self._isToken) return (1us + sub_len.0, sub_len.1, sub_len.2);
                    else return (0us, false, []);                
                }
            }

            if (self._isToken) return (1us, self._isSkip, self._isComment);
            return (0us, false, []);
        }

        
        /**
         * @returns: the key of the node
         */
        fn key (self) -> T {
            self._key
        }
        
        impl Streamable {
            /**
             * For debug purposes, tokenizer Nodes are printable
             */
            pub over toStream (self, dmut stream : &StringStream) {
                self.innerPrint (0, alias stream);
            }
        }

        /**
         * Debug print of the node on stdout
         */
        prv fn innerPrint (self, i : i32, dmut stream : &StringStream) {
            for _ in 0 .. i { stream:.write ("\t"s8); }            
            stream:.write ('['c8):.write (self._key):.write (']'c8):.write (self._isToken);
            for _, j in self._heads {                
                j.innerPrint (i + 1, alias stream);                
            }            
        }


        impl Copiable;
    }

    
}



